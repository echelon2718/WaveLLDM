{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(\n",
    "            qkv, \"b (qkv heads c) h w -> qkv b heads c (h w)\", heads=self.heads, qkv=3\n",
    "        )\n",
    "        \n",
    "        # Apply ELU activation to q and k\n",
    "        q = self.elu(q) + 1  # Ensure positivity\n",
    "        k = self.elu(k) + 1  # Ensure positivity\n",
    "\n",
    "        # Attention mechanism\n",
    "        context = torch.einsum(\"bhdn,bhen->bhde\", k, v)\n",
    "        out = torch.einsum(\"bhde,bhdn->bhen\", context, q)\n",
    "        \n",
    "        # Reshape and project out\n",
    "        out = rearrange(\n",
    "            out, \"b heads c (h w) -> b (heads c) h w\", heads=self.heads, h=h, w=w\n",
    "        )\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialSelfAttention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "        self.Q = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.K = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.V = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_out = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        Q = self.Q(h_)\n",
    "        K = self.K(h_)\n",
    "        V = self.V(h_)\n",
    "\n",
    "        B, C, H, W = Q.shape\n",
    "        Q = rearrange(Q, 'b c h w -> b (h w) c')\n",
    "        K = rearrange(K, 'b c h w -> b (h w) c')\n",
    "        context_ = torch.einsum('bij,bjk->bik', Q, K)\n",
    "\n",
    "        context_ = context_ * (int(C) ** -0.5)\n",
    "        context_ = F.softmax(context_, dim=2)\n",
    "\n",
    "        V = rearrange(V, 'b c h w -> b c (h w)')\n",
    "        context_ = rearrange(context_, 'b i j -> b j i')\n",
    "        h_ = torch.einsum('bij,bjk->bik', V, context_)\n",
    "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=H, w=W)\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x + h_\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    use_flash_attention = False\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0, is_inplace=True):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        if context_dim is None:\n",
    "            context_dim = query_dim\n",
    "        \n",
    "        # QKV Mappings\n",
    "        d_attn = dim_head * heads\n",
    "        self.Q = nn.Linear(query_dim, d_attn, bias=False)\n",
    "        self.K = nn.Linear(context_dim, d_attn, bias=False)\n",
    "        self.V = nn.Linear(context_dim, d_attn, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(d_attn, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Trying to use flash attention if exist:\n",
    "        try:\n",
    "            from flash_attn.flash_attention import FlashAttention\n",
    "\n",
    "            self.flash = FlashAttention()\n",
    "            self.flash.softmax_scale = self.scale\n",
    "        except ImportError:\n",
    "            self.flash = None\n",
    "    \n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        has_cond = context is not None\n",
    "        if not has_cond:\n",
    "            context = x\n",
    "        \n",
    "        Q = self.Q(x)\n",
    "        K = self.K(context)\n",
    "        V = self.V(context)\n",
    "        print(\"Berhasil\")\n",
    "        if (\n",
    "            CrossAttention.use_flash_attention\n",
    "            and self.flash is not None\n",
    "            and not has_cond\n",
    "            and self.dim_head <= 128\n",
    "        ):\n",
    "            return self.flash_attention(Q, K, V)\n",
    "        else:\n",
    "            return self.standard_attention(Q, K, V)\n",
    "    \n",
    "    def flash_attention(self, Q, K, V):\n",
    "        B, N, C  = Q.shape\n",
    "        QKV = torch.stack((Q, K, V), dim=2)\n",
    "        QKV = QKV.view(B, N, 3, self.heads, self.dim_head)\n",
    "\n",
    "        if self.d_head <= 32:\n",
    "            pad = 32 - self.d_head\n",
    "        elif self.d_head <= 64:\n",
    "            pad = 64 - self.d_head\n",
    "        elif self.d_head <= 128:\n",
    "            pad = 128 - self.d_head\n",
    "        else:\n",
    "            raise ValueError(f\"dim_head {self.d_head} not supported: Too large for Flash Attention\")\n",
    "        \n",
    "        if pad:\n",
    "            QKV = torch.cat(\n",
    "                (QKV, QKV.new_zeros(B, N, 3, self.heads, pad)), dim=-1\n",
    "            )\n",
    "        \n",
    "        out, _ = self.flash(QKV.type(torch.float16))\n",
    "        out = out[:, :, :, :self.dim_head].float()\n",
    "        out = out.reshape(B, N, self.heads * self.dim_head)\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "    def standard_attention(self, Q, K, V):\n",
    "        B, N, C = Q.shape\n",
    "        Q = rearrange(Q, 'b n (h d) -> b n h d', h=self.heads)\n",
    "        K = rearrange(K, 'b m (h d) -> b h m d', h=self.heads)\n",
    "        V = rearrange(V, 'b m (h d) -> b h m d', h=self.heads)\n",
    "\n",
    "        context = torch.einsum('bqhd,bhmd->bqhm', Q, K) * self.scale\n",
    "\n",
    "        # if self.is_inplace:\n",
    "        #     half = context.shape[0]//2\n",
    "        #     context[half:] = context[half:].softmax(dim=-1)\n",
    "        #     context[:half] = context[:half].softmax(dim=-1)\n",
    "\n",
    "        context = F.softmax(context, dim=-1)\n",
    "        out = torch.einsum('bqhm,bhmd->bqhd', context, V)\n",
    "        out = rearrange(out, 'b n h d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = torch.randn(1, 3, 32, 32)\n",
    "model = LinearAttention(3)\n",
    "out = model(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = CrossAttention(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_in_emb = torch.randn(1, 256, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2(sample_in_emb, sample_in_emb).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
