{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modules import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import models\n",
    "from models.modules import get_extra_padding_for_conv1d\n",
    "from models.unet import DiffusionUNet, create_diffusion_model\n",
    "from models.utils import LogMelSpectrogram, count_parameters, get_padding_sample\n",
    "from models.discriminator import EnsembleDiscriminator\n",
    "from training.dataset_vctk import DenoiserDataset, collate_fn_latents\n",
    "from models.lldm_architecture import WaveLLDM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "backbone = models.ConvNeXtEncoder(\n",
    "    input_channels=160,\n",
    "    depths=[3, 3, 9, 3],\n",
    "    dims=[128, 256, 384, 512],\n",
    "    drop_path_rate=0.2,\n",
    "    kernel_size=7\n",
    ").to(device)\n",
    "\n",
    "head = models.HiFiGANGenerator(\n",
    "    hop_length=512,\n",
    "    upsample_rates=[8, 8, 2, 2, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4, 4],\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    num_mels=512,\n",
    "    upsample_initial_channel=512,\n",
    "    pre_conv_kernel_size=13,\n",
    "    post_conv_kernel_size=13\n",
    ").to(device)\n",
    "\n",
    "quantizer = models.DownsampleFSQ(\n",
    "    input_dim=512,\n",
    "    n_groups=8,\n",
    "    n_codebooks=8,\n",
    "    levels=[8, 8, 8, 6, 5],\n",
    "    downsample_factor=[2, 2]\n",
    ").to(device)\n",
    "\n",
    "spec_trans = LogMelSpectrogram(\n",
    "    sample_rate=44100,\n",
    "    n_mels=160,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    win_length=2048\n",
    ").to(device)\n",
    "\n",
    "ffgan = models.FireflyArchitecture(\n",
    "    backbone=backbone,\n",
    "    head=head,\n",
    "    quantizer=quantizer,\n",
    "    spec_transform=spec_trans\n",
    ")\n",
    "\n",
    "unet = create_diffusion_model(\n",
    "    in_channels=1024,\n",
    "    base_channels=32,\n",
    "    out_channels=512\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Putra Santoso\\AppData\\Local\\Temp\\ipykernel_30900\\2718160704.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ffgan.load_state_dict(torch.load(\"./pretrained_models/generator_step_142465.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 1st-stage model state dicts\n",
    "ffgan.load_state_dict(torch.load(\"./pretrained_models/generator_step_142465.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_trans_cpu = LogMelSpectrogram(\n",
    "    sample_rate=44100,\n",
    "    n_mels=160,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    win_length=2048\n",
    ").to(\"cpu\")\n",
    "\n",
    "encoder_cpu = models.ConvNeXtEncoder(\n",
    "    input_channels=160,\n",
    "    depths=[3, 3, 9, 3],\n",
    "    dims=[128, 256, 384, 512],\n",
    "    drop_path_rate=0.2,\n",
    "    kernel_size=7\n",
    ").to(\"cpu\")\n",
    "\n",
    "quantizer_cpu = models.DownsampleFSQ(\n",
    "    input_dim=512,\n",
    "    n_groups=8,\n",
    "    n_codebooks=8,\n",
    "    levels=[8, 8, 8, 6, 5],\n",
    "    downsample_factor=[2, 2]\n",
    ").to(\"cpu\")\n",
    "\n",
    "decoder_cpu = models.HiFiGANGenerator(\n",
    "    hop_length=512,\n",
    "    upsample_rates=[8, 8, 2, 2, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4, 4],\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    num_mels=512,\n",
    "    upsample_initial_channel=512,\n",
    "    pre_conv_kernel_size=13,\n",
    "    post_conv_kernel_size=13\n",
    ").to(\"cpu\")\n",
    "\n",
    "encoder_cpu.load_state_dict(ffgan.backbone.state_dict())\n",
    "quantizer_cpu.load_state_dict(ffgan.quantizer.state_dict())\n",
    "decoder_cpu.load_state_dict(ffgan.head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DenoiserDataset(\n",
    "    \"./data/voicebank_demand_56spk/clean_speech_audios/train/\",\n",
    "    \"./data/voicebank_demand_56spk/noisy_speech_audios/train/\",\n",
    "    True,\n",
    "    stage=3,\n",
    "    spec_trans=spec_trans_cpu,\n",
    "    encoder=encoder_cpu,\n",
    "    quantizer=quantizer_cpu,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "val_ds = DenoiserDataset(\n",
    "    \"./data/voicebank_demand_56spk/clean_speech_audios/test/\",\n",
    "    \"./data/voicebank_demand_56spk/noisy_speech_audios/test/\",\n",
    "    True,\n",
    "    stage=3,\n",
    "    spec_trans=spec_trans_cpu,\n",
    "    encoder=encoder_cpu,\n",
    "    quantizer=quantizer_cpu,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelldm = WaveLLDM(\n",
    "    p_estimator=unet,\n",
    "    learn_logvar=False,\n",
    "    encoder=ffgan.backbone,\n",
    "    quantizer=ffgan.quantizer,\n",
    "    decoder=ffgan.head,\n",
    "    beta_scheduler=\"cosine\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_latents\n",
    ")\n",
    "\n",
    "latent_val_dataloader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_latents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "Loss:  1.2705508470535278\n",
      "{'train/loss_simple': tensor(1.2627, device='cuda:0', grad_fn=<MeanBackward0>), 'train/loss_vlb': tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>), 'train/loss': tensor(1.2706, device='cuda:0', grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(latent_train_dataloader):\n",
    "    # print(data[\"melspec_lengths\"][0])\n",
    "    # loss, loss_dict = wavelldm(data)\n",
    "    # print(\"Loss: \", loss.item())\n",
    "    # print(loss_dict)\n",
    "\n",
    "    wavelldm.log_reconstruction(Summary)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "class WaveLLDMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: WaveLLDM,\n",
    "        train_dataloader,\n",
    "        val_dataloader=None,\n",
    "        epochs: int = 300,\n",
    "        save_dir: str = \"./checkpoints\",\n",
    "        log_dir: str = \"./logs\",\n",
    "        save_every: int = 5,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.save_dir = save_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.save_every = save_every\n",
    "        self.device = device\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_steps = 0\n",
    "\n",
    "            with tqdm(total=len(self.train_dataloader), desc=f\"Epoch {epoch+1}/{self.epochs}\", unit=\"batch\") as pbar:\n",
    "                for idx, batch in enumerate(self.train_dataloader):\n",
    "                    loss, loss_dict = self.model.train_step(batch)\n",
    "                    train_loss += loss.item()\n",
    "                    train_steps += 1\n",
    "\n",
    "                    avg_train_loss_on_fly = train_loss / (idx + 1)\n",
    "\n",
    "                    pbar.set_postfix({\n",
    "                        'Loss': avg_train_loss_on_fly\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if idx % 100 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                    \n",
    "                    self.model.log_to_tensorboard(\n",
    "                        self.writer, loss_dict, train_steps + epoch * len(self.train_dataloader), prefix=\"train\", batch=batch\n",
    "                    )\n",
    "            \n",
    "            avg_train_loss = train_loss / train_steps\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            if self.val_dataloader is not None:\n",
    "                val_loss = self.model.validate(epoch, self.val_dataloader, self.writer)\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Average Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if (epoch + 1) % self.save_every == 0:\n",
    "                self.model.save_checkpoint(epoch + 1)\n",
    "        \n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_wavelldm = WaveLLDMTrainer(\n",
    "    model=wavelldm,\n",
    "    train_dataloader=latent_train_dataloader,\n",
    "    val_dataloader=latent_val_dataloader,\n",
    "    epochs=300,\n",
    "    save_dir=\"./checkpoints\",\n",
    "    log_dir=\"./logs/wavelldm\",\n",
    "    save_every=5,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180cd19beed4ab7bde2c98702eeb026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/300:   0%|          | 0/722 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer_wavelldm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m, in \u001b[0;36mWaveLLDMTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader):\n\u001b[0;32m     39\u001b[0m         loss, loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_step(batch)\n\u001b[0;32m     40\u001b[0m         train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Projects\\WaveLLDM\\training\\dataset_vctk.py:189\u001b[0m, in \u001b[0;36mDenoiserDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    186\u001b[0m     noisy_audio_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(noisy_audio_spec\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# Quantize latent audio\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m     zq_down_clean_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_audio_latent\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlatents\n\u001b[0;32m    190\u001b[0m     zq_down_noisy_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(noisy_audio_latent)\u001b[38;5;241m.\u001b[39mlatents\n\u001b[0;32m    192\u001b[0m item \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzq_down_clean_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m: zq_down_clean_audio\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzq_down_noisy_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m: zq_down_noisy_audio\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoisy_sr_default\u001b[39m\u001b[38;5;124m\"\u001b[39m: noisy_sr_new\n\u001b[0;32m    199\u001b[0m }\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Projects\\WaveLLDM\\models\\fsq.py:85\u001b[0m, in \u001b[0;36mDownsampleFSQ.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     83\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     84\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(z)\n\u001b[1;32m---> 85\u001b[0m quantized, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_fsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m result \u001b[38;5;241m=\u001b[39m FSQResult(\n\u001b[0;32m     87\u001b[0m     z\u001b[38;5;241m=\u001b[39mquantized\u001b[38;5;241m.\u001b[39mmT,\n\u001b[0;32m     88\u001b[0m     codes\u001b[38;5;241m=\u001b[39mindices\u001b[38;5;241m.\u001b[39mmT,\n\u001b[0;32m     89\u001b[0m     latents\u001b[38;5;241m=\u001b[39mz,\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     91\u001b[0m result\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(result\u001b[38;5;241m.\u001b[39mz)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\residual_fsq.py:313\u001b[0m, in \u001b[0;36mGroupedResidualFSQ.forward\u001b[1;34m(self, x, return_all_codes)\u001b[0m\n\u001b[0;32m    306\u001b[0m forward_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    307\u001b[0m     return_all_codes \u001b[38;5;241m=\u001b[39m return_all_codes,\n\u001b[0;32m    308\u001b[0m     rand_quantize_dropout_fixed_seed \u001b[38;5;241m=\u001b[39m get_maybe_sync_seed(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# invoke residual vq on each group\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrvq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrvq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mout))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# otherwise, get all the zipped outputs and combine them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\residual_fsq.py:313\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    306\u001b[0m forward_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    307\u001b[0m     return_all_codes \u001b[38;5;241m=\u001b[39m return_all_codes,\n\u001b[0;32m    308\u001b[0m     rand_quantize_dropout_fixed_seed \u001b[38;5;241m=\u001b[39m get_maybe_sync_seed(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# invoke residual vq on each group\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(rvq(chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m rvq, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrvqs, x))\n\u001b[0;32m    314\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mout))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# otherwise, get all the zipped outputs and combine them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\residual_fsq.py:208\u001b[0m, in \u001b[0;36mResidualFSQ.forward\u001b[1;34m(self, x, return_all_codes, rand_quantize_dropout_fixed_seed)\u001b[0m\n\u001b[0;32m    205\u001b[0m     all_indices\u001b[38;5;241m.\u001b[39mappend(null_indices)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m quantized, indices \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m quantized \u001b[38;5;241m=\u001b[39m quantized \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m    212\u001b[0m residual \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m-\u001b[39m quantized\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\finite_scalar_quantization.py:252\u001b[0m, in \u001b[0;36mFSQ.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    249\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices:\n\u001b[1;32m--> 252\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodes_to_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m codes \u001b[38;5;241m=\u001b[39m rearrange(codes, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n c d -> b n (c d)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    256\u001b[0m codes \u001b[38;5;241m=\u001b[39m codes\u001b[38;5;241m.\u001b[39mto(orig_dtype)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\finite_scalar_quantization.py:183\u001b[0m, in \u001b[0;36mFSQ.codes_to_indices\u001b[1;34m(self, zhat)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Converts a `code` to an index in the codebook. \"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m zhat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook_dim\n\u001b[1;32m--> 183\u001b[0m zhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_and_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzhat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (zhat \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basis)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(int32)\n",
      "File \u001b[1;32mc:\\KevinsAnaconda\\envs\\new_mytorch\\lib\\site-packages\\vector_quantize_pytorch\\finite_scalar_quantization.py:169\u001b[0m, in \u001b[0;36mFSQ._scale_and_shift\u001b[1;34m(self, zhat_normalized)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_scale_and_shift\u001b[39m(\u001b[38;5;28mself\u001b[39m, zhat_normalized):\n\u001b[0;32m    168\u001b[0m     half_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_levels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mzhat_normalized\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhalf_width\u001b[49m) \u001b[38;5;241m+\u001b[39m half_width\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_wavelldm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
