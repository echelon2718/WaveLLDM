{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645c64c4-509d-4966-b835-d8c191b568f5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c53e9ee-3eec-428c-a979-978230f82375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/riset/.local/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/riset/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/riset/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/riset/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/riset/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/riset/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/riset/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/riset/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/riset/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, safetensors, requests, regex, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.27.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5da8dee-33fe-4bf1-989a-4bab38dbec98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d95f1b0f462475b982cbdb44f90507f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, IterableDataset, IterableDatasetDict, load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../data/hifi_tts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a90247-a036-4c23-a0ce-b18deba3ab3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['speaker', 'file', 'duration', 'text', 'text_no_preprocessing', 'text_normalized', 'audio'],\n",
       "    num_rows: 125989\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6c83a-a3dd-4324-ae5c-b68b77e04590",
   "metadata": {},
   "source": [
    "## Model Firefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564be1fb-fc02-4c67-9257-4eb2ff1232b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from math import prod\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from torch.nn.utils.parametrize import remove_parametrizations\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6085dd8-8a9f-4914-a0a9-ae7f9e5de7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(length, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0b318f-a375-499a-8953-68f50d43f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv1D\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return (kernel_size * dilation - dilation) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fff184-067b-4a96-92ef-afa72ce1cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad1d(x: torch.Tensor, paddings: tuple[int, int]):\n",
    "    \"\"\"Remove padding from x, handling properly zero padding. Only for 1d!\"\"\"\n",
    "    padding_left, padding_right = paddings\n",
    "    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)\n",
    "    assert (padding_left + padding_right) <= x.shape[-1]\n",
    "    end = x.shape[-1] - padding_right\n",
    "    return x[..., padding_left:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a2ba3d-4e82-4737-b069-a350dfac2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_padding_for_conv1d(\n",
    "    x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0\n",
    ") -> int:\n",
    "    \"\"\"See `pad_for_conv1d`.\"\"\"\n",
    "    length = x.shape[-1]\n",
    "    n_frames = (length - kernel_size + padding_total) / stride + 1\n",
    "    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)\n",
    "    return ideal_length - length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86afd9e-08d9-49c4-9092-6e1be183c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad1d(\n",
    "    x: torch.Tensor,\n",
    "    paddings: tuple[int, int],\n",
    "    mode: str = \"zeros\",\n",
    "    value: float = 0.0,\n",
    "):\n",
    "    \"\"\"Tiny wrapper around F.pad, just to allow for reflect padding on small input.\n",
    "    If this is the case, we insert extra 0 padding to the right\n",
    "    before the reflection happen.\n",
    "    \"\"\"\n",
    "    length = x.shape[-1]\n",
    "    padding_left, padding_right = paddings\n",
    "    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)\n",
    "    if mode == \"reflect\":\n",
    "        max_pad = max(padding_left, padding_right)\n",
    "        extra_pad = 0\n",
    "        if length <= max_pad:\n",
    "            extra_pad = max_pad - length + 1\n",
    "            x = F.pad(x, (0, extra_pad))\n",
    "        padded = F.pad(x, paddings, mode, value)\n",
    "        end = padded.shape[-1] - extra_pad\n",
    "        return padded[..., :end]\n",
    "    else:\n",
    "        return F.pad(x, paddings, mode, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759dce19-ae11-417d-9f16-7d90710d3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, stride=1, groups=1):\n",
    "        super(ConvolutionalNet, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "        )\n",
    "        self.stride = stride\n",
    "        self.kernel_size = (kernel_size - 1) * dilation + 1\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        pad = self.kernel_size - self.stride\n",
    "        extra_padding = get_extra_padding_for_conv1d(\n",
    "            x, self.kernel_size, self.stride, pad\n",
    "        )\n",
    "        x = pad1d(x, (pad, extra_padding), mode=\"constant\", value = 0) # Cari tahu value ini apa\n",
    "        return self.conv(x).contiguous()\n",
    "\n",
    "    def weight_norm(self, name=\"weight\", dim = 0):\n",
    "        self.conv = weight_norm(self.conv, name=name, dim=dim)\n",
    "        return self\n",
    "\n",
    "    def remove_parametrizations(self, name=\"weight\"):\n",
    "        self.conv = remove_parametrizations(self.conv, name)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8748e137-3f3c-466f-a078-02442e5fa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, stride=1):\n",
    "        super(TransConvNet, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation\n",
    "        )\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        pad = self.kernel_size - self.stride\n",
    "        padding_right = math.ceil(pad)\n",
    "        padding_left = pad - padding_right\n",
    "        x = unpad1d(x, (padding_left, padding_right))\n",
    "        return x.contiguous()\n",
    "\n",
    "    def weight_norm(self, name=\"weight\", dim=0):\n",
    "        self.conv = weight_norm(self.conv, name=name, dim=dim)\n",
    "        return self\n",
    "\n",
    "    def remove_parametrizations(self, name=\"weight\"):\n",
    "        self.conv = remove_parametrizations(self.conv, name)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c288fa9-a2f3-4914-bffc-63fbd9956f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock1(torch.nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs1 = nn.ModuleList(\n",
    "            [\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[0]\n",
    "                ).weight_norm(),\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[1]\n",
    "                ).weight_norm(),\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[2]\n",
    "                ).weight_norm(),\n",
    "            ]\n",
    "        )\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList(\n",
    "            [\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[0]\n",
    "                ).weight_norm(),\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[1]\n",
    "                ).weight_norm(),\n",
    "                ConvolutionalNet(\n",
    "                    channels, channels, kernel_size, stride=1, dilation=dilation[2]\n",
    "                ).weight_norm(),\n",
    "            ]\n",
    "        )\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.silu(x)\n",
    "            xt = c1(xt)\n",
    "            xt = F.silu(xt)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_parametrizations(self):\n",
    "        for conv in self.convs1:\n",
    "            conv.remove_parametrizations()\n",
    "        for conv in self.convs2:\n",
    "            conv.remove_parametrizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce275d8-b917-4f6f-8cf6-1b017d6d9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        kernel_sizes: tuple[int] = (3, 7, 11),\n",
    "        dilation_sizes: tuple[tuple[int]] = ((1, 3, 5), (1, 3, 5), (1, 3, 5)),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_sizes) == len(dilation_sizes)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for k, d in zip(kernel_sizes, dilation_sizes):\n",
    "            self.blocks.append(ResBlock1(channels, k, d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.stack([block(x) for block in self.blocks], dim=0).mean(dim=0)\n",
    "\n",
    "    def remove_parametrizations(self):\n",
    "        for block in self.blocks:\n",
    "            block.remove_parametrizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d73a56-8116-4db9-82ff-5d0a411f4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiFiGANGenerator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 hop_length: int = 512,\n",
    "                 upsample_rates: tuple[int] = (8, 8, 2, 2, 2),\n",
    "                 upsample_kernel_sizes: tuple[int] = (16, 16, 8, 2, 2),\n",
    "                 resblock_kernel_sizes: tuple[int] = (3, 7, 11),\n",
    "                 resblock_dilation_sizes: tuple[tuple[int]] = ((1, 3, 5), (1, 3, 5), (1, 3, 5)),\n",
    "                 num_mels: int = 128,\n",
    "                 upsample_initial_channel: int = 512,\n",
    "                 pre_conv_kernel_size: int = 7,\n",
    "                 post_conv_kernel_size: int = 7,\n",
    "                 post_activation: Callable = partial(nn.SiLU, inplace=True)\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (\n",
    "            prod(upsample_rates) == hop_length\n",
    "        ), f\"hop_length must be {prod(upsample_rates)}\"\n",
    "\n",
    "        self.conv_pre = ConvolutionalNet(\n",
    "            num_mels,\n",
    "            upsample_initial_channel,\n",
    "            pre_conv_kernel_size,\n",
    "            stride = 1\n",
    "        ).weight_norm()\n",
    "\n",
    "        self.num_upsamples = len(upsample_rates)\n",
    "        self.num_kernels = len(resblock_kernel_sizes)\n",
    "\n",
    "        self.noise_convs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(\n",
    "                TransConvNet(\n",
    "                    upsample_initial_channel // (2**i),\n",
    "                    upsample_initial_channel // (2**(i + 1)),\n",
    "                    k,\n",
    "                    stride=u\n",
    "                ).weight_norm()\n",
    "            )\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2 ** (i + 1))\n",
    "            self.resblocks.append(\n",
    "                ParallelBlock(ch, resblock_kernel_sizes, resblock_dilation_sizes)\n",
    "            )\n",
    "\n",
    "        self.activation_post = post_activation()\n",
    "\n",
    "        self.conv_post = ConvolutionalNet(\n",
    "            ch, 1, post_conv_kernel_size, stride=1\n",
    "        ).weight_norm()\n",
    "\n",
    "        self.ups.apply(init_weights)\n",
    "        self.conv_post.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.silu(x, inplace=True)\n",
    "            x = self.ups[i](x)\n",
    "\n",
    "            # if self.training and self.checkpointing:\n",
    "            #     x = checkpoint(\n",
    "            #         self.resblocks[i],\n",
    "            #         x,\n",
    "            #         use_reentrant=False,\n",
    "            #     )\n",
    "            # else:\n",
    "            #     x = self.resblocks[i](x)\n",
    "\n",
    "            x = self.resblocks[i](x)\n",
    "\n",
    "        x = self.activation_post(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def remove_parametrizations(self):\n",
    "        for up in self.ups:\n",
    "            up.remove_parametrizations()\n",
    "        for block in self.resblocks:\n",
    "            block.remove_parametrizations()\n",
    "        self.conv_pre.remove_parametrizations()\n",
    "        self.conv_post.remove_parametrizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb1fc37-3f1b-4861-9d6f-7413f55fee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(\n",
    "    x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True\n",
    "):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (\n",
    "        x.ndim - 1\n",
    "    )  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e67023a8-0a9f-4d95-99ad-12cd6a5bacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, drop_prob: float = 0.0, scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"drop_prob={round(self.drop_prob,3):0.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe23b4b-a52f-4113-a95c-d53184395e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(\n",
    "                x, self.normalized_shape, self.weight, self.bias, self.eps\n",
    "            )\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None] * x + self.bias[:, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6215eff-decf-45fd-9a8f-28535f29a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock(nn.Module):\n",
    "    r\"\"\"ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.\n",
    "        kernel_size (int): Kernel size for depthwise conv. Default: 7.\n",
    "        dilation (int): Dilation for depthwise conv. Default: 1.\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        drop_path: float = 0.0,\n",
    "        layer_scale_init_value: float = 1e-6,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        kernel_size: int = 7,\n",
    "        dilation: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dwconv = ConvolutionalNet(\n",
    "            dim,\n",
    "            dim,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=dim,\n",
    "        )  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(\n",
    "            dim, int(mlp_ratio * dim)\n",
    "        )  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(int(mlp_ratio * dim), dim)\n",
    "        self.gamma = (\n",
    "            nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "            if layer_scale_init_value > 0\n",
    "            else None\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, apply_residual: bool = True):\n",
    "        input = x\n",
    "\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 1)  # (N, C, L) -> (N, L, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (N, L, C) -> (N, C, L)\n",
    "        x = self.drop_path(x)\n",
    "\n",
    "        if apply_residual:\n",
    "            x = input + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56ff803f-fd78-4e12-8df6-6acee5a4ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        depths: list[int] = [3, 3, 9, 3],\n",
    "        dims: list[int] = [96, 192, 384, 768],\n",
    "        drop_path_rate: float = 0.0,\n",
    "        layer_scale_init_value: float = 1e-6,\n",
    "        kernel_size: int = 7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(depths) == len(dims)\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            ConvolutionalNet(\n",
    "                input_channels,\n",
    "                dims[0],\n",
    "                kernel_size=7,\n",
    "            ),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        for i in range(len(depths) - 1):\n",
    "            mid_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv1d(dims[i], dims[i + 1], kernel_size=1),\n",
    "            )\n",
    "            self.downsample_layers.append(mid_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        cur = 0\n",
    "        for i in range(len(depths)):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    ConvNeXtBlock(\n",
    "                        dim=dims[i],\n",
    "                        drop_path=dp_rates[cur + j],\n",
    "                        layer_scale_init_value=layer_scale_init_value,\n",
    "                        kernel_size=kernel_size,\n",
    "                    )\n",
    "                    for j in range(depths[i])\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = LayerNorm(dims[-1], eps=1e-6, data_format=\"channels_first\")\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        for i in range(len(self.downsample_layers)):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fcc9591-5412-4a0c-8cf6-7ecedb8d697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireflyArchitecture(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        head: nn.Module,\n",
    "        quantizer: nn.Module,\n",
    "        spec_transform: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self.quantizer = quantizer\n",
    "        self.spec_transform = spec_transform\n",
    "        self.downsample_factor = math.prod(self.quantizer.downsample_factor)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, template=None, mask=None) -> torch.Tensor:\n",
    "        if self.spec_transform is not None:\n",
    "            x = self.spec_transform(x)\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        if mask is not None:\n",
    "            x = x * mask\n",
    "\n",
    "        if self.quantizer is not None:\n",
    "            vq_result = self.quantizer(x)\n",
    "            x = vq_result.z\n",
    "\n",
    "            if mask is not None:\n",
    "                x = x * mask\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        if x.ndim == 2:\n",
    "            x = x[:, None, :]\n",
    "\n",
    "        return x, vq_result\n",
    "\n",
    "    def encode(self, audios, audio_lengths):\n",
    "        audios = audios.float()\n",
    "\n",
    "        mels = self.spec_transform(audios)\n",
    "        mel_lengths = audio_lengths // self.spec_transform.hop_length\n",
    "        mel_masks = sequence_mask(mel_lengths, mels.shape[2])\n",
    "        mel_masks_float_conv = mel_masks[:, None, :].float()\n",
    "        mels = mels * mel_masks_float_conv\n",
    "\n",
    "        # Encode\n",
    "        encoded_features = self.backbone(mels) * mel_masks_float_conv\n",
    "        feature_lengths = mel_lengths // self.downsample_factor\n",
    "\n",
    "        return self.quantizer.encode(encoded_features), feature_lengths\n",
    "\n",
    "    def decode(self, indices, feature_lengths) -> torch.Tensor:\n",
    "        mel_masks = sequence_mask(\n",
    "            feature_lengths * self.downsample_factor,\n",
    "            indices.shape[2] * self.downsample_factor,\n",
    "        )\n",
    "        mel_masks_float_conv = mel_masks[:, None, :].float()\n",
    "        audio_lengths = (\n",
    "            feature_lengths * self.downsample_factor * self.spec_transform.hop_length\n",
    "        )\n",
    "\n",
    "        audio_masks = sequence_mask(\n",
    "            audio_lengths,\n",
    "            indices.shape[2] * self.downsample_factor * self.spec_transform.hop_length,\n",
    "        )\n",
    "        audio_masks_float_conv = audio_masks[:, None, :].float()\n",
    "\n",
    "        z = self.quantizer.decode(indices) * mel_masks_float_conv\n",
    "        x = self.head(z) * audio_masks_float_conv\n",
    "\n",
    "        return x, audio_lengths\n",
    "\n",
    "    def remove_parametrizations(self):\n",
    "        if hasattr(self.backbone, \"remove_parametrizations\"):\n",
    "            self.backbone.remove_parametrizations()\n",
    "\n",
    "        if hasattr(self.head, \"remove_parametrizations\"):\n",
    "            self.head.remove_parametrizations()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318005ac-4430-4f6c-bcd3-f23b2eb906d4",
   "metadata": {},
   "source": [
    "## GFSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca6c3ecb-74fa-461d-bd56-91f6faf7937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vector_quantize_pytorch in /home/riset/.local/lib/python3.10/site-packages (1.20.11)\n",
      "Requirement already satisfied: einops>=0.8.0 in /home/riset/.local/lib/python3.10/site-packages (from vector_quantize_pytorch) (0.8.0)\n",
      "Requirement already satisfied: einx>=0.3.0 in /home/riset/.local/lib/python3.10/site-packages (from vector_quantize_pytorch) (0.3.0)\n",
      "Requirement already satisfied: torch>=2.0 in /home/riset/.local/lib/python3.10/site-packages (from vector_quantize_pytorch) (2.1.2+cu121)\n",
      "Requirement already satisfied: numpy in /home/riset/.local/lib/python3.10/site-packages (from einx>=0.3.0->vector_quantize_pytorch) (1.26.4)\n",
      "Requirement already satisfied: sympy in /home/riset/.local/lib/python3.10/site-packages (from einx>=0.3.0->vector_quantize_pytorch) (1.12)\n",
      "Requirement already satisfied: frozendict in /home/riset/.local/lib/python3.10/site-packages (from einx>=0.3.0->vector_quantize_pytorch) (2.4.6)\n",
      "Requirement already satisfied: filelock in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (4.10.0)\n",
      "Requirement already satisfied: networkx in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/riset/.local/lib/python3.10/site-packages (from torch>=2.0->vector_quantize_pytorch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/riset/.local/lib/python3.10/site-packages (from jinja2->torch>=2.0->vector_quantize_pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/riset/.local/lib/python3.10/site-packages (from sympy->einx>=0.3.0->vector_quantize_pytorch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vector_quantize_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc99fe21-f9ec-4874-ab88-76fdf83e652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFIKASI\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from vector_quantize_pytorch import GroupedResidualFSQ\n",
    "\n",
    "@dataclass\n",
    "class FSQResult:\n",
    "    # Represents quantization outputs\n",
    "    z: torch.Tensor        \n",
    "    codes: torch.Tensor    \n",
    "    latents: torch.Tensor  \n",
    "\n",
    "class DownsampleFSQ(nn.Module):\n",
    "    \"\"\"Performs downsampling and FSQ quantization with configurable architecture\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim=512,\n",
    "        n_codebooks=9,\n",
    "        n_groups=1,\n",
    "        levels=(8, 5, 5, 5),\n",
    "        downsample_factor=(2, 2),\n",
    "        downsample_dims=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Handle default case for downsample dimensions\n",
    "        self.downsample_dims = downsample_dims or [input_dim] * len(downsample_factor)\n",
    "        self.dims_sequence = [input_dim] + list(self.downsample_dims)\n",
    "        \n",
    "        # Configure quantizer\n",
    "        self._setup_quantizer(self.dims_sequence[-1], levels, n_codebooks, n_groups)\n",
    "        \n",
    "        # Build network components\n",
    "        self._build_downsample_network(downsample_factor)\n",
    "        self._build_upsample_network(downsample_factor)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.downsample_factor = downsample_factor\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_network_weights()\n",
    "\n",
    "    def _setup_quantizer(self, dim, levels, n_codebooks, n_groups):\n",
    "        \"\"\"Configures the FSQ quantizer with specified parameters\"\"\"\n",
    "        self.residual_fsq = GroupedResidualFSQ(\n",
    "            dim=dim,\n",
    "            levels=levels,\n",
    "            num_quantizers=n_codebooks,\n",
    "            groups=n_groups\n",
    "        )\n",
    "\n",
    "    def _build_downsample_network(self, factors):\n",
    "        \"\"\"Constructs the downsampling pathway\"\"\"\n",
    "        layers = []\n",
    "        for idx in range(len(factors)):\n",
    "            conv_block = self._create_conv_block(\n",
    "                self.dims_sequence[idx],\n",
    "                self.dims_sequence[idx + 1],\n",
    "                factors[idx]\n",
    "            )\n",
    "            layers.append(conv_block)\n",
    "        self.downsample = nn.Sequential(*layers)\n",
    "\n",
    "    def _build_upsample_network(self, factors):\n",
    "        \"\"\"Constructs the upsampling pathway\"\"\"\n",
    "        layers = []\n",
    "        for idx in reversed(range(len(factors))):\n",
    "            conv_block = self._create_conv_block(\n",
    "                self.dims_sequence[idx + 1],\n",
    "                self.dims_sequence[idx],\n",
    "                factors[idx]\n",
    "            )\n",
    "            layers.append(conv_block)\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "\n",
    "    def _create_conv_block(self, in_dim, out_dim, factor):\n",
    "        \"\"\"Creates a single convolutional block\"\"\"\n",
    "        return nn.Sequential(\n",
    "            ConvolutionalNet(in_dim, out_dim, kernel_size=factor, stride=factor),\n",
    "            ConvNeXtBlock(dim=out_dim)\n",
    "        )\n",
    "\n",
    "    def _initialize_network_weights(self):\n",
    "        \"\"\"Initializes network weights using truncated normal distribution\"\"\"\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def _adjust_output_size(self, tensor, target_size):\n",
    "        \"\"\"Adjusts tensor size through padding or trimming\"\"\"\n",
    "        current_size = tensor.shape[-1]\n",
    "        size_diff = target_size - current_size\n",
    "        \n",
    "        if size_diff == 0:\n",
    "            return tensor\n",
    "            \n",
    "        if size_diff > 0:\n",
    "            pad_left = size_diff // 2\n",
    "            pad_right = size_diff - pad_left\n",
    "            return F.pad(tensor, (pad_left, pad_right))\n",
    "        else:\n",
    "            trim_left = -size_diff // 2\n",
    "            trim_right = current_size - (-size_diff - trim_left)\n",
    "            return tensor[..., trim_left:trim_right]\n",
    "\n",
    "    def forward(self, z) -> FSQResult:\n",
    "        # Store original dimensions\n",
    "        target_size = z.shape[-1]\n",
    "        \n",
    "        # Process through networks\n",
    "        compressed = self.downsample(z)\n",
    "        quantized, indices = self.residual_fsq(compressed.mT)\n",
    "        \n",
    "        # Create initial result\n",
    "        result = FSQResult(\n",
    "            z=quantized.mT,\n",
    "            codes=indices.mT,\n",
    "            latents=compressed\n",
    "        )\n",
    "        \n",
    "        # Upsample and adjust dimensions\n",
    "        result.z = self.upsample(result.z)\n",
    "        result.z = self._adjust_output_size(result.z, target_size)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def encode(self, z):\n",
    "        compressed = self.downsample(z)\n",
    "        _, indices = self.residual_fsq(compressed.mT)\n",
    "        return rearrange(indices, \"g b l r -> b (g r) l\")\n",
    "\n",
    "    def decode(self, indices):\n",
    "        # Reshape indices for processing\n",
    "        grouped_indices = rearrange(\n",
    "            indices, \n",
    "            \"b (g r) l -> g b l r\", \n",
    "            g=self.residual_fsq.groups\n",
    "        )\n",
    "        \n",
    "        # Reconstruct signal\n",
    "        quantized = self.residual_fsq.get_output_from_indices(grouped_indices)\n",
    "        return self.upsample(quantized.mT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3089ab5f-7837-4468-aada-8b6cf3d88097",
   "metadata": {},
   "source": [
    "## U-Net untuk LLDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2d93f-f7eb-49db-9c22-443a594dec89",
   "metadata": {},
   "source": [
    "Arsitektur U-Net yang diusulkan akan menggunakan 4 level network, dimana setiap level akan terdiri atas 2 residual network, dan 1 attention layer (pada middle block strukturnya R-A-R, R: Residual, A: Attention).\n",
    "\n",
    "U-Net ini juga perlu memproses timestep ke dalam embedding. Attention mechanism yang digunakan akan memuat rotational embedding, ini berperan penting untuk menangkap dan mempelajari fitur temporal (acuan berasal dari LLaMA). Selain itu, sistem Attentionnya akan berupa Self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed72a74c-52c7-42db-bac0-2d89ab456435",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ConvNeXtEncoder(\n",
    "    input_channels = 160,\n",
    "    depths = [3, 3, 9, 3],\n",
    "    dims = [128, 256, 288, 384],\n",
    "    drop_path_rate = 0.2,\n",
    "    kernel_size = 7\n",
    ")\n",
    "head = HiFiGANGenerator(\n",
    "    hop_length=512,\n",
    "    upsample_rates=[8, 8, 2, 2, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4, 4],\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "    num_mels = 384, #default 512\n",
    "    upsample_initial_channel=384, #default 512\n",
    "    pre_conv_kernel_size=13,\n",
    "    post_conv_kernel_size=13\n",
    ")\n",
    "quantizer = DownsampleFSQ(\n",
    "    input_dim=384,\n",
    "    n_groups=8,\n",
    "    n_codebooks=1,\n",
    "    levels=[8, 5, 5, 5],\n",
    "    downsample_factor=[2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7eda64eb-b670-4937-a3c3-482bb3a65696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85045248"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in ResBlock1(channels=512).parameters()) * 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3e1b5b2-da50-432a-84b1-3ba2dd3fde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FireflyArchitecture(\n",
    "    backbone=backbone,\n",
    "    head=head,\n",
    "    quantizer=quantizer,\n",
    "    spec_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2bd067f-1c79-4411-9a4d-0b3d7071e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, vq = model(torch.randn(1, 160, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d8bad18-3ff0-4405-9132-3657bc969d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = backbone(torch.randn(1, 160, 256))\n",
    "quantized = quantizer(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c5fe43d-0323-4b5e-bad3-05d2662de400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 131072])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(quantized.z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24946343-ce40-443a-be97-389da24274c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = backbone(torch.randn(1, 160, 124))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b23d0da3-332a-4db9-9f0b-cba4884832fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 124])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv1d(in_channels=768, out_channels=768, kernel_size=3, stride=1, padding=1)(intermediate).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0f637-ca6a-4476-b385-2529e5424e51",
   "metadata": {},
   "source": [
    "intermediate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5576c838-cf32-4557-913e-3eaae95acaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._norm(x.float()).type_as(x) * self.scale\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Time embedding using sinusoidal positions, similar to transformer positional encoding.\n",
    "    This helps the model understand the diffusion timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=x.device) * -embeddings)\n",
    "        embeddings = x[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return self.mlp(embeddings)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling block with dual residual blocks and optional attention\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, time_dim, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.res1 = ResBlock1(in_channels)\n",
    "        self.res2 = ResBlock1(in_channels)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        \n",
    "        if use_attention:\n",
    "            # Convert channels to sequence for attention\n",
    "            self.norm = RMSNorm(out_channels)\n",
    "            self.attention = RotaryAttention(\n",
    "                ModelArgs(dim=out_channels, n_heads=8, n_kv_heads=4),\n",
    "                dim=out_channels,\n",
    "                n_heads=8,\n",
    "                n_kv_heads=4,\n",
    "                max_seq_len=2048\n",
    "            )\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.downsample(x)\n",
    "        \n",
    "        if self.attention is not None:\n",
    "            # Reshape for attention: [B, C, L] -> [B, L, C]\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = self.attention(x)\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block with dual residual blocks and optional attention\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, time_dim, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.res1 = ResBlock1(in_channels)\n",
    "        self.res2 = ResBlock1(in_channels)\n",
    "        self.upsample = TransConvNet(in_channels, out_channels, 4, stride=2)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.norm = RMSNorm(out_channels)\n",
    "            self.attention = RotaryAttention(\n",
    "                ModelArgs(dim=out_channels, n_heads=8, n_kv_heads=4),\n",
    "                dim=out_channels,\n",
    "                n_heads=8,\n",
    "                n_kv_heads=4,\n",
    "                max_seq_len=2048\n",
    "            )\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    def forward(self, x, skip_x=None):\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        if skip_x is not None:\n",
    "            x = torch.cat([x, skip_x], dim=1)\n",
    "            \n",
    "        if self.attention is not None:\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = self.attention(x)\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class DiffusionUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced U-Net architecture with rotary attention and residual blocks.\n",
    "    Features 4 levels with dual residual blocks and attention integration.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=4,\n",
    "        model_channels=64,\n",
    "        out_channels=4,\n",
    "        time_dim=None,\n",
    "        num_levels=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_dim = time_dim or model_channels * 4\n",
    "        time_dim = self.time_dim\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = SinusoidalTimeEmbedding(model_channels)\n",
    "        \n",
    "        # Initial projection\n",
    "        self.init_conv = ConvolutionalNet(in_channels, model_channels, kernel_size=3)\n",
    "        \n",
    "        # Down blocks - increasing channel dimensions\n",
    "        ch = model_channels\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        channels = []\n",
    "        \n",
    "        for level in range(num_levels):\n",
    "            use_attention = level > 0  # Use attention from second level onwards\n",
    "            out_ch = ch * 2\n",
    "            \n",
    "            self.down_blocks.append(\n",
    "                DownBlock(ch, out_ch, time_dim, use_attention)\n",
    "            )\n",
    "            channels.append(ch)\n",
    "            ch = out_ch\n",
    "        \n",
    "        # Middle block with Residual-Attention-Residual structure\n",
    "        self.mid_block1 = ResBlock1(ch)\n",
    "        self.mid_attn = RotaryAttention(\n",
    "            ModelArgs(dim=ch, n_heads=8, n_kv_heads=4),\n",
    "            dim=ch,\n",
    "            n_heads=8,\n",
    "            n_kv_heads=4,\n",
    "            max_seq_len=2048\n",
    "        )\n",
    "        self.mid_block2 = ResBlock1(ch)\n",
    "        \n",
    "        # Up blocks - decreasing channel dimensions\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for level in reversed(range(num_levels)):\n",
    "            use_attention = level > 0  # Match down block attention\n",
    "            in_ch = ch + channels.pop()  # Add skip connection channels\n",
    "            out_ch = ch // 2\n",
    "            \n",
    "            self.up_blocks.append(\n",
    "                UpBlock(in_ch, out_ch, time_dim, use_attention)\n",
    "            )\n",
    "            ch = out_ch\n",
    "            \n",
    "        # Final blocks\n",
    "        self.final_res = ResBlock1(ch)\n",
    "        self.final_conv = ConvolutionalNet(ch, out_channels, kernel_size=3)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, x, time):\n",
    "        # Time embedding\n",
    "        t = self.time_mlp(time)\n",
    "        \n",
    "        # Initial projection\n",
    "        x = self.init_conv(x)\n",
    "        \n",
    "        # Store skip connections\n",
    "        skips = []\n",
    "        \n",
    "        # Down path\n",
    "        for block in self.down_blocks:\n",
    "            skips.append(x)\n",
    "            x = block(x)\n",
    "        \n",
    "        # Middle block (R-A-R)\n",
    "        x = self.mid_block1(x)\n",
    "        # Prepare for attention\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.mid_attn(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.mid_block2(x)\n",
    "        \n",
    "        # Up path with skip connections\n",
    "        for block in self.up_blocks:\n",
    "            skip_x = skips.pop()\n",
    "            x = block(x, skip_x)\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.final_res(x)\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_diffusion_model(\n",
    "    in_channels=4,\n",
    "    base_channels=64,\n",
    "    out_channels=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a diffusion model with the specified configuration\n",
    "    \"\"\"\n",
    "    return DiffusionUNet(\n",
    "        in_channels=in_channels,\n",
    "        model_channels=base_channels,\n",
    "        out_channels=out_channels,\n",
    "        num_levels=4  # 4 levels as specified\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a94122a0-c0a2-49e8-b808-fe5ead73abd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RotaryAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 233\u001b[0m, in \u001b[0;36mcreate_diffusion_model\u001b[0;34m(in_channels, base_channels, out_channels)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_diffusion_model\u001b[39m(\n\u001b[1;32m    226\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    227\u001b[0m     base_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m    228\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    229\u001b[0m ):\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Creates a diffusion model with the specified configuration\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDiffusionUNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 4 levels as specified\u001b[39;49;00m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 150\u001b[0m, in \u001b[0;36mDiffusionUNet.__init__\u001b[0;34m(self, in_channels, model_channels, out_channels, time_dim, num_levels)\u001b[0m\n\u001b[1;32m    146\u001b[0m use_attention \u001b[38;5;241m=\u001b[39m level \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Use attention from second level onwards\u001b[39;00m\n\u001b[1;32m    147\u001b[0m out_ch \u001b[38;5;241m=\u001b[39m ch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mDownBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    152\u001b[0m channels\u001b[38;5;241m.\u001b[39mappend(ch)\n\u001b[1;32m    153\u001b[0m ch \u001b[38;5;241m=\u001b[39m out_ch\n",
      "Cell \u001b[0;32mIn[33], line 54\u001b[0m, in \u001b[0;36mDownBlock.__init__\u001b[0;34m(self, in_channels, out_channels, time_dim, use_attention)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_attention:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Convert channels to sequence for attention\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(out_channels)\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mRotaryAttention\u001b[49m(\n\u001b[1;32m     55\u001b[0m         ModelArgs(dim\u001b[38;5;241m=\u001b[39mout_channels, n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, n_kv_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     56\u001b[0m         dim\u001b[38;5;241m=\u001b[39mout_channels,\n\u001b[1;32m     57\u001b[0m         n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     58\u001b[0m         n_kv_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     59\u001b[0m         max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RotaryAttention' is not defined"
     ]
    }
   ],
   "source": [
    "model = create_diffusion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e303185-cf2c-42e1-85b7-3948f736537e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203c4da-af63-40af-85a7-e5407f8ba445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
